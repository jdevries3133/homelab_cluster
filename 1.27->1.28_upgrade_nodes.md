Upgrade process;

1. Upgrade a primary control plane node.
2. Upgrade additional control plane nodes.
3. Upgrade worker nodes.

# [Before you Begin](https://v1-28.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)

- [x] Make sure you read the release notes carefully.
- [x] The cluster should use a static control plane and etcd pods or external
  etcd.
- [x] Make sure to back up any important components, such as app-level state
  stored in a database. kubeadm upgrade does not touch your workloads, only
  components internal to Kubernetes, but backups are always a best practice.
- [x] Swap must be disabled (check `sudo cat /proc/swaps`).
- [x] Migrate to the new package repository [following this
  guide](https://v1-28.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/change-package-repository/)
  (this step is added by me)

# Release Notes Notes

I'm using a very vanilla set of K8s APIs;

```
=== v1/Pod ===
calcount-deployment-d47c7cc5d-98psm, calcount-deployment-d47c7cc5d-rwghr, ...
==============

=== v1/Service ===
calcount-service, db-postgresql, db-postgresql-hl, calico-api, ...
==================

=== apps/v1/DaemonSet ===
calico-node, csi-node-driver, kube-proxy, nginx-ingress-nginx-ingress, openebs-ndm
=========================

=== apps/v1/Deployment ===
calcount-deployment, calico-apiserver, calico-kube-controllers, ...
==========================

=== apps/v1/ReplicaSet ===
calcount-deployment-5467fb76c9, calcount-deployment-5697b55864, ...
==========================

=== apps/v1/StatefulSet ===
db-postgresql, db-postgresql, db-postgresql, db-postgresql, ...
==========================
```

This means that I should be prepared to upgrade with no drama, since I'm not
using deprecated APIs.

# Package Repository Migration

As a dependency for this upgrade, I need to [move to the new Ubuntu
repository](https://v1-28.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/change-package-repository/).

# [Upgrading Control-Plane Node](https://v1-28.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#upgrading-control-plane-nodes)

This was very painless. I just finished step 4, "Choose a version to upgrade to,
and run the appropriate command." My control plane is now running 1.28! It turns
out that if you remove the control plane, existing workloads are unaffected,
they just cannot change and will not be controlled (i.e, if a pod crashes,
nothing will step in to restart it). This is pretty slick, because with a single
control-plane like I have, I can take the control plane fully offline, update
it, and then bring it back, without interrupting any existing workloads.

# [Upgrade Worker Nodes](https://v1-28.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#upgrade-worker-nodes)

I am not sure how OpenEBS HostPath volumes (created using the `local-ssd`
StorageClass) are going to fare with node upgrades. I realized that the `nick`
node has databases for my notion-clone and phat-stack projects, which
coincidentally are ones I don't really care about. This means that I can
experiment with these guys first. I think that in general, this is why people
like to have replicated storage.

## PV Manual Backups

It turns out that when we try to drain the nodes, we get a complaint;

```
âžœ  homelab_cluster git:(main) âœ— k drain nick --ignore-daemonsets
node/nick already cordoned
error: unable to drain node "nick" due to error:cannot delete Pods with local storage (use --delete-emptydir-data to override): nc/db-postgresql-0, phat-stack/db-postgresql-0, continuing command...
There are pending nodes to be drained:
 nick
cannot delete Pods with local storage (use --delete-emptydir-data to override): nc/db-postgresql-0, phat-stack/db-postgresql-0
```

This makes sense; `kubelet` will delete the data in use when we try to drain.
It's become increasingly clear that LocalStorage is meant to be used as a
scratch-pad, not for backing databases ðŸ˜…

It looks like we'll need to delete and then manually re-create these guys, so
here's the procedure I'll use for a migration which creates downtime, but
preserves data;

1. Scale the postgresql deployments to zero replicas so that nothing is touching
   the persistent volume data.
2. Backup the PV to a safe location on the host node, so that we preserve the
   data after deletion (i.e, `sudo cp -R /mnt/openebs-ssd/pvc-* backups`).
3. Drain the node; the change to zero replicas for the psql deployment will
   persist, so a new PV will be created on the remaining node, but we don't care
   about that for now.
4. Upgrade the now-drained worker.
5. Restore the data for Postgres (maybe on a different node now, wherever the
   PVC ended up being satisfied).
6. Scale the database deployment back up from zero to one.
7. ??? everything should work

Note: I just did a little test of step 1, and then reversing step 1, which
works, and that's heartening!

Note: it's critical to ensure that file permissions match as the backup is moved
into the newly created PV. `tar` will save the ownership information of files in
the archive, but write files as the current user by default. To expand with the
exact users from the archive, use the `--same-owner`, and `--same-permissions`
flags;

```
sudo tar --same-owner --same-permissions -xzvf backup.tgz
```

Next steps -- do this whole migration plan to upgrade the `nick` node.

## dweedledum upgrade

After performing the above procedure on `nick`, most PVs are now running on
dweedledum, so let's make some notes for the upgrade;

| Path                                                      | App      |
| --------------------------------------------------------- | -------- |
| /mnt/openebs-ssd/pvc-b66e41be-b160-459e-943d-92965334b6a7 | calcount |
| /mnt/openebs-ssd/pvc-a9cbf98c-6010-4648-a689-9d0c209cb4a1 | jdv      |
| /mnt/openebs-ssd/pvc-70653872-e088-48c8-98e3-fc9aa6522b7e | nc       |

The only DB among these that we actually care about is `calcount`. We'll allow
the remainder to be deleted and re-created.
